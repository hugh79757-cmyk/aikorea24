#!/usr/bin/env python3
"""ì •ë¶€ ê³µë¬¸ì„œ AI í•™ìŠµë°ì´í„° ìˆ˜ì§‘ê¸° - aikorea24 ì—°ë™"""

import os
import sys
import json
import requests
import subprocess
import hashlib
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸
PROJECT_ROOT = '/Users/twinssn/Projects/aikorea24'

# .envì—ì„œ í‚¤ ë¡œë“œ
env_path = os.path.join(PROJECT_ROOT, '.env')
if os.path.exists(env_path):
    with open(env_path) as f:
        for line in f:
            line = line.strip()
            if '=' in line and not line.startswith('#'):
                k, v = line.split('=', 1)
                os.environ[k.strip()] = v.strip()

API_KEY = os.environ.get('DATA_GO_KR_KEY', '')
if not API_KEY:
    print('ERROR: DATA_GO_KR_KEY not found in .env')
    sys.exit(1)

BASE_URL = 'http://apis.data.go.kr/1741000/publicDoc'

# AI ê´€ë ¨ ê²€ìƒ‰ í‚¤ì›Œë“œ
SEARCH_KEYWORDS = [
    'AI', 'ì¸ê³µì§€ëŠ¥', 'ë””ì§€í„¸', 'ë°ì´í„°', 'í´ë¼ìš°ë“œ',
    'ì†Œí”„íŠ¸ì›¨ì–´', 'ì‚¬ì´ë²„', 'ìŠ¤ë§ˆíŠ¸', 'ììœ¨ì£¼í–‰', 'ë¡œë´‡',
    'ë°˜ë„ì²´', 'GPT', 'ë”¥ëŸ¬ë‹', 'ë¹…ë°ì´í„°', 'ë©”íƒ€ë²„ìŠ¤'
]

# ì—”ë“œí¬ì¸íŠ¸ë³„ ë¬¸ì„œ ìœ í˜•
ENDPOINTS = {
    'getDocPress': 'ë³´ë„ìë£Œ',
    'getDocReport': 'ì •ì±…ë³´ê³ ì„œ',
    'getDocSpeech': 'ì—°ì„¤ë¬¸',
}

# AI ê´€ë ¨ í•„í„°
STRONG_AI_WORDS = ['AI', 'ì¸ê³µì§€ëŠ¥', 'GPT', 'ë”¥ëŸ¬ë‹', 'ë¨¸ì‹ ëŸ¬ë‹', 'LLM', 'ìƒì„±í˜•', 'ì±—ë´‡', 'ChatGPT', 'í´ë¡œë“œ', 'ì•¤íŠ¸ë¡œí”½', 'OpenAI']
WEAK_AI_WORDS = ['ë””ì§€í„¸', 'ë°ì´í„°', 'í´ë¼ìš°ë“œ', 'ìŠ¤ë§ˆíŠ¸', 'ììœ¨ì£¼í–‰', 'ë¡œë´‡', 'ë°˜ë„ì²´', 'ì†Œí”„íŠ¸ì›¨ì–´', 'ì‚¬ì´ë²„', 'ë¹…ë°ì´í„°', 'ë©”íƒ€ë²„ìŠ¤', 'í”Œë«í¼']
EXCLUDE_WORDS = ['ê·€ë†', 'ê·€ì´Œ', 'ê·€ì–´', 'êµë³µ', 'ë¶€ë™ì‚°', 'ì•„íŒŒíŠ¸', 'ì¶•êµ¬', 'ì•¼êµ¬', 'ê²°í˜¼', 'ì´í˜¼', 'ì¥ë¡€']

def is_ai_related(title, text_preview=''):
    combined = (title + ' ' + text_preview[:200]).upper()
    for w in EXCLUDE_WORDS:
        if w in combined:
            return False
    for w in STRONG_AI_WORDS:
        if w.upper() in combined:
            return True
    weak_count = sum(1 for w in WEAK_AI_WORDS if w.upper() in combined)
    return weak_count >= 2

def fetch_docs(endpoint, keyword, num=10, page=1):
    url = f'{BASE_URL}/{endpoint}'
    params = {
        'serviceKey': API_KEY,
        'format': 'json',
        'numOfRows': num,
        'pageNo': page,
        'title': keyword
    }
    try:
        resp = requests.get(url, params=params, timeout=15)
        data = resp.json()
        body = data.get('response', {}).get('body', {})
        total = body.get('totalCount', 0)
        results = body.get('resultList', [])
        if isinstance(results, dict):
            results = [results]
        return results, total
    except Exception as e:
        print(f'  Error fetching {endpoint}/{keyword}: {e}')
        return [], 0

def safe_sql(text, maxlen=500):
    if not text:
        return ''
    t = text.replace("'", "''").replace('\\', '').replace('\n', ' ').replace('\r', '')
    return t[:maxlen]

def title_hash(title):
    return hashlib.md5(title.strip().lower().encode()).hexdigest()

def get_existing_hashes():
    cmd = f'npx wrangler d1 execute aikorea24-db --remote --command "SELECT title FROM news;"'
    try:
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, cwd=PROJECT_ROOT, timeout=30)
        hashes = set()
        for line in result.stdout.split('\n'):
            line = line.strip()
            if line and not line.startswith(('â›…', 'â”€', 'ğŸŒ€', 'âœ˜', 'Resource', 'â”Œ', 'â”‚', 'â”œ', 'â””', 'Getting')):
                hashes.add(title_hash(line))
        return hashes
    except:
        return set()

def insert_to_d1(item):
    title = safe_sql(item['title'], 200)
    link = safe_sql(item.get('link', ''), 500)
    desc = safe_sql(item.get('description', ''), 1000)
    source = safe_sql(item.get('source', ''), 50)
    category = safe_sql(item.get('category', 'policy'), 20)
    pub_date = safe_sql(item.get('pub_date', ''), 30)

    sql = f"INSERT INTO news (title, link, description, source, category, pub_date) VALUES ('{title}', '{link}', '{desc}', '{source}', '{category}', '{pub_date}');"
    cmd = f'npx wrangler d1 execute aikorea24-db --remote --command "{sql}"'
    try:
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, cwd=PROJECT_ROOT, timeout=15)
        return result.returncode == 0 and 'ERROR' not in result.stderr
    except:
        return False

def main():
    print('=' * 60)
    print(f'ì •ë¶€ ê³µë¬¸ì„œ AI í•™ìŠµë°ì´í„° ìˆ˜ì§‘ê¸°')
    print(f'ì‹¤í–‰ ì‹œê°: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}')
    print('=' * 60)

    # ê¸°ì¡´ ë‰´ìŠ¤ í•´ì‹œ
    print('\nê¸°ì¡´ ë‰´ìŠ¤ í•´ì‹œ ë¡œë”©...')
    existing = get_existing_hashes()
    print(f'ê¸°ì¡´ ë‰´ìŠ¤: {len(existing)}ê±´')

    collected = []
    for endpoint, doc_type in ENDPOINTS.items():
        print(f'\n--- {doc_type} ({endpoint}) ---')
        ep_count = 0
        for kw in SEARCH_KEYWORDS:
            results, total = fetch_docs(endpoint, kw, num=10, page=1)
            if not results:
                continue
            for item in results:
                if isinstance(item, dict) and 'meta' in item:
                    meta = item['meta']
                    text_data = item.get('data', {}).get('text', '')
                else:
                    meta = item
                    text_data = ''

                title = meta.get('title', '')
                if not title:
                    continue
                if not is_ai_related(title, text_data):
                    continue
                if title_hash(title) in existing:
                    continue

                # ë¯¸ë¦¬ë³´ê¸° í…ìŠ¤íŠ¸ (ì• 300ì)
                preview = text_data[:300].replace("'", "''") if text_data else ''

                collected.append({
                    'title': title,
                    'link': f'https://www.data.go.kr/data/15125451/openapi.do',
                    'description': preview if preview else f'{doc_type} - {meta.get("ministry", "")} ({meta.get("date", "")})',
                    'source': f'ì •ë¶€ê³µë¬¸ì„œ({doc_type})',
                    'category': 'policy',
                    'pub_date': meta.get('date', '')
                })
                existing.add(title_hash(title))
                ep_count += 1
        print(f'  {doc_type} AI ê´€ë ¨: {ep_count}ê±´')

    print(f'\nì´ ì‹ ê·œ ìˆ˜ì§‘: {len(collected)}ê±´')

    if not collected:
        print('ì‹ ê·œ ë°ì´í„° ì—†ìŒ. ì¢…ë£Œ.')
        return

    # D1 ì €ì¥
    print('\nD1 ì €ì¥ ì¤‘...')
    saved = 0
    failed = 0
    for i, item in enumerate(collected):
        if insert_to_d1(item):
            saved += 1
            print(f'  [{i+1}/{len(collected)}] ì €ì¥: {item["title"][:40]}...')
        else:
            failed += 1
            print(f'  [{i+1}/{len(collected)}] ì‹¤íŒ¨: {item["title"][:40]}...')

    print(f'\nì™„ë£Œ: ì €ì¥ {saved}ê±´, ì‹¤íŒ¨ {failed}ê±´')

if __name__ == '__main__':
    main()
