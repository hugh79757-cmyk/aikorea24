#!/usr/bin/env python3
"""
aikorea24.kr - ì •ë¶€ AI íŠ¹í™” API í…ŒìŠ¤íŠ¸
ì‹¤í–‰: python3 test_gov_ai_apis.py
"""

import os
import json
import requests
from datetime import datetime
from bs4 import BeautifulSoup

results = []
def log(name, status, detail=""):
    emoji = "âœ…" if status == "OK" else "âŒ" if status == "FAIL" else "âš ï¸"
    results.append({"api": name, "status": status, "detail": detail})
    print(f"  {emoji} [{name}] {detail}")

print("=" * 60)
print(f"ì •ë¶€ AI íŠ¹í™” API í…ŒìŠ¤íŠ¸: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print("=" * 60)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# [1] ì •ì±…ë¸Œë¦¬í•‘ - ê³¼ê¸°ì •í†µë¶€ RSS (í‚¤ ë¶ˆí•„ìš”)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\nğŸ“¡ [1/6] ì •ì±…ë¸Œë¦¬í•‘ ê³¼ê¸°ì •í†µë¶€ RSS")
try:
    url = "https://www.korea.kr/rss/dept_msit.xml"
    r = requests.get(url, timeout=10)
    if r.status_code == 200:
        soup = BeautifulSoup(r.text, 'xml')
        items = soup.find_all('item')
        ai_items = []
        for item in items:
            title = item.find('title').get_text(strip=True) if item.find('title') else ""
            if any(kw in title for kw in ['AI', 'ì¸ê³µì§€ëŠ¥', 'ë°ì´í„°', 'ë””ì§€í„¸', 'ICT']):
                ai_items.append(title[:60])
        log("ê³¼ê¸°ì •í†µë¶€ RSS", "OK",
            f"ì „ì²´ {len(items)}ê±´, AIê´€ë ¨ {len(ai_items)}ê±´")
        for t in ai_items[:3]:
            print(f"    â†’ {t}")
    else:
        log("ê³¼ê¸°ì •í†µë¶€ RSS", "FAIL", f"HTTP {r.status_code}")
except Exception as e:
    log("ê³¼ê¸°ì •í†µë¶€ RSS", "FAIL", str(e))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# [2] ì •ì±…ë¸Œë¦¬í•‘ - ì •ì±…ë‰´ìŠ¤ RSS (AI í•„í„°)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\nğŸ“¡ [2/6] ì •ì±…ë¸Œë¦¬í•‘ ì •ì±…ë‰´ìŠ¤ RSS (AI í•„í„°)")
try:
    url = "https://www.korea.kr/rss/policy.xml"
    r = requests.get(url, timeout=10)
    if r.status_code == 200:
        soup = BeautifulSoup(r.text, 'xml')
        items = soup.find_all('item')
        ai_items = []
        for item in items:
            title = item.find('title').get_text(strip=True) if item.find('title') else ""
            desc = item.find('description').get_text(strip=True) if item.find('description') else ""
            link = item.find('link').get_text(strip=True) if item.find('link') else ""
            combined = title + " " + desc
            if any(kw in combined for kw in ['AI', 'ì¸ê³µì§€ëŠ¥', 'ë°”ìš°ì²˜', 'ë°ì´í„°', 'ë””ì§€í„¸ì „í™˜', 'GPT', 'ìƒì„±í˜•']):
                ai_items.append({"title": title[:60], "link": link})
        log("ì •ì±…ë‰´ìŠ¤ RSS", "OK",
            f"ì „ì²´ {len(items)}ê±´, AIê´€ë ¨ {len(ai_items)}ê±´")
        for item in ai_items[:3]:
            print(f"    â†’ {item['title']}")
    else:
        log("ì •ì±…ë‰´ìŠ¤ RSS", "FAIL", f"HTTP {r.status_code}")
except Exception as e:
    log("ì •ì±…ë‰´ìŠ¤ RSS", "FAIL", str(e))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# [3] ë³´ì¡°ê¸ˆí†µí•©í¬í„¸ - ê³µëª¨ì‚¬ì—… API
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\nğŸ’° [3/6] ë³´ì¡°ê¸ˆí†µí•©í¬í„¸ ê³µëª¨ì‚¬ì—… API")
try:
    api_key = os.environ.get("DATA_GO_KR_KEY", "")
    if not api_key:
        log("ë³´ì¡°ê¸ˆ ê³µëª¨ì‚¬ì—…", "SKIP", "DATA_GO_KR_KEY ë¯¸ì„¤ì •")
    else:
        # ê³µëª¨ì‚¬ì—… ëª©ë¡ API
        url = "https://apis.data.go.kr/B552468/srchFrnrSbsdMng/getOpenBizList"
        params = {
            "serviceKey": api_key,
            "pageNo": "1",
            "numOfRows": "20",
            "type": "json"
        }
        r = requests.get(url, params=params, timeout=15)
        if r.status_code == 200:
            try:
                data = r.json()
                # AI ê´€ë ¨ ê³µëª¨ì‚¬ì—… í•„í„°ë§
                items = data.get("response", {}).get("body", {}).get("items", [])
                if isinstance(items, dict):
                    items = items.get("item", [])
                ai_items = [i for i in items
                           if any(kw in str(i) for kw in ['AI', 'ì¸ê³µì§€ëŠ¥', 'ë°”ìš°ì²˜', 'ë°ì´í„°'])]
                log("ë³´ì¡°ê¸ˆ ê³µëª¨ì‚¬ì—…", "OK",
                    f"ì „ì²´ {len(items)}ê±´, AIê´€ë ¨ {len(ai_items)}ê±´")
            except:
                log("ë³´ì¡°ê¸ˆ ê³µëª¨ì‚¬ì—…", "WARN", f"JSON íŒŒì‹± ì‹¤íŒ¨: {r.text[:100]}")
        else:
            log("ë³´ì¡°ê¸ˆ ê³µëª¨ì‚¬ì—…", "FAIL", f"HTTP {r.status_code}")
except Exception as e:
    log("ë³´ì¡°ê¸ˆ ê³µëª¨ì‚¬ì—…", "FAIL", str(e))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# [4] NIPA ì‚¬ì—…ê³µê³  í¬ë¡¤ë§ (AI ë°”ìš°ì²˜)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\nğŸ“‹ [4/6] NIPA ì‚¬ì—…ê³µê³  í¬ë¡¤ë§ (AI ë°”ìš°ì²˜)")
try:
    r = requests.get("https://www.nipa.kr/home/2-2",
                     timeout=10, headers={"User-Agent": "Mozilla/5.0"})
    if r.status_code == 200:
        soup = BeautifulSoup(r.text, 'html.parser')
        ai_notices = []
        for tag in soup.find_all(['a', 'td', 'span']):
            text = tag.get_text(strip=True)
            if any(kw in text for kw in ['AI', 'ì¸ê³µì§€ëŠ¥', 'ë°”ìš°ì²˜', 'ë°ì´í„°']):
                href = tag.get('href', '')
                if text and len(text) > 5 and text not in [t['title'] for t in ai_notices]:
                    ai_notices.append({"title": text[:70], "href": href})
        unique = {item['title']: item for item in ai_notices}
        log("NIPA ì‚¬ì—…ê³µê³ ", "OK", f"AI ê´€ë ¨ {len(unique)}ê±´")
        for title in list(unique.keys())[:5]:
            print(f"    â†’ {title}")
    else:
        log("NIPA ì‚¬ì—…ê³µê³ ", "FAIL", f"HTTP {r.status_code}")
except Exception as e:
    log("NIPA ì‚¬ì—…ê³µê³ ", "FAIL", str(e))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# [5] IRIS R&D ì‚¬ì—…ê³µê³  í¬ë¡¤ë§ (AI ê³¼ì œ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\nğŸ”¬ [5/6] IRIS R&D ì‚¬ì—…ê³µê³  (AI ê³¼ì œ)")
try:
    url = "https://www.iris.go.kr/contents/retrieveBsnsAncmBtinSituListView.do"
    r = requests.get(url, timeout=10, headers={"User-Agent": "Mozilla/5.0"})
    if r.status_code == 200:
        soup = BeautifulSoup(r.text, 'html.parser')
        all_text = soup.get_text()
        ai_count = sum(1 for kw in ['AI', 'ì¸ê³µì§€ëŠ¥', 'ë°ì´í„°', 'ë””ì§€í„¸']
                      if kw in all_text)
        log("IRIS ì‚¬ì—…ê³µê³ ", "OK",
            f"í˜ì´ì§€ ë¡œë“œ ì„±ê³µ ({len(r.text):,}bytes), AIí‚¤ì›Œë“œ {ai_count}ì¢… ë°œê²¬")
    else:
        log("IRIS ì‚¬ì—…ê³µê³ ", "FAIL", f"HTTP {r.status_code}")
except Exception as e:
    log("IRIS ì‚¬ì—…ê³µê³ ", "FAIL", str(e))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# [6] AIí—ˆë¸Œ ì˜¤í”ˆAPI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\nğŸ§  [6/6] AIí—ˆë¸Œ ë°ì´í„°ì…‹ í˜ì´ì§€")
try:
    r = requests.get("https://www.aihub.or.kr/aihubdata/data/view.do",
                     timeout=10, headers={"User-Agent": "Mozilla/5.0"})
    if r.status_code == 200:
        soup = BeautifulSoup(r.text, 'html.parser')
        # ë°ì´í„°ì…‹ ìˆ˜ ì¶”ì¶œ ì‹œë„
        text = soup.get_text()
        log("AIí—ˆë¸Œ ë°ì´í„°ì…‹", "OK",
            f"í˜ì´ì§€ ë¡œë“œ ì„±ê³µ ({len(r.text):,}bytes)")

        aihub_key = os.environ.get("AIHUB_API_KEY", "")
        if aihub_key:
            log("AIí—ˆë¸Œ APIí‚¤", "OK", "AIHUB_API_KEY ì„¤ì •ë¨")
        else:
            log("AIí—ˆë¸Œ APIí‚¤", "SKIP",
                "AIHUB_API_KEY ë¯¸ì„¤ì • (https://aihub.or.kr ê°€ì… í›„ ë°œê¸‰)")
    else:
        log("AIí—ˆë¸Œ ë°ì´í„°ì…‹", "FAIL", f"HTTP {r.status_code}")
except Exception as e:
    log("AIí—ˆë¸Œ ë°ì´í„°ì…‹", "FAIL", str(e))

# â”€â”€â”€ ìµœì¢… ë¦¬í¬íŠ¸ â”€â”€â”€
print("\n" + "=" * 60)
print("ğŸ“Š ì •ë¶€ AI API í…ŒìŠ¤íŠ¸ ê²°ê³¼")
print("=" * 60)
ok = sum(1 for r in results if r["status"] == "OK")
fail = sum(1 for r in results if r["status"] == "FAIL")
skip = sum(1 for r in results if r["status"] == "SKIP")
warn = sum(1 for r in results if r["status"] == "WARN")
print(f"  âœ… ì„±ê³µ: {ok}  âŒ ì‹¤íŒ¨: {fail}  âš ï¸ ê²½ê³ : {warn}  â­ï¸ ìŠ¤í‚µ: {skip}")

print(f"""
â”â”â”â” aikorea24.kr ì „ì²´ API í™˜ê²½ë³€ìˆ˜ ëª©ë¡ â”â”â”â”

[í•„ìˆ˜ - ì¦‰ì‹œ ë°œê¸‰ ê°€ëŠ¥]
  NAVER_CLIENT_ID        â† developers.naver.com (ê²€ìƒ‰+ë°ì´í„°ë©)
  NAVER_CLIENT_SECRET    â† ìœ„ì™€ ë™ì¼
  OPENAI_API_KEY         â† platform.openai.com
  DATA_GO_KR_KEY         â† data.go.kr (ë³´ì¡°ê¸ˆ+ê³µê³µì„œë¹„ìŠ¤)

[í•„ìˆ˜ - ê´‘ê³  ê³„ì • í•„ìš”]
  NAVER_AD_API_KEY       â† manage.searchad.naver.com
  NAVER_AD_SECRET        â† ìœ„ì™€ ë™ì¼
  NAVER_AD_CUSTOMER_ID   â† ìœ„ì™€ ë™ì¼

[ì„ íƒ - ë‚˜ì¤‘ì— ë°œê¸‰]
  AIHUB_API_KEY          â† aihub.or.kr (AI ë°ì´í„°ì…‹)
  INSTAGRAM_ACCESS_TOKEN â† developers.facebook.com
  INSTAGRAM_BUSINESS_ID  â† ìœ„ì™€ ë™ì¼
  GOOGLE_TRENDS_API_KEY  â† ê³µì‹ API ì•ŒíŒŒ (ëŒ€ê¸° ì¤‘)

[í‚¤ ë¶ˆí•„ìš” - ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥]
  ì •ì±…ë¸Œë¦¬í•‘ RSS (ê³¼ê¸°ì •í†µë¶€)  â† korea.kr/rss/dept_msit.xml
  ì •ì±…ë¸Œë¦¬í•‘ RSS (ì •ì±…ë‰´ìŠ¤)    â† korea.kr/rss/policy.xml
  ì •ì±…ë¸Œë¦¬í•‘ RSS (ë³´ë„ìë£Œ)    â† korea.kr/rss/pressrelease.xml
  Google Trends RSS          â† trends.google.co.kr/trending/rss?geo=KR
  NIPA ì‚¬ì—…ê³µê³                â† nipa.kr/home/2-2 (í¬ë¡¤ë§)
  IRIS R&D ê³µê³                â† iris.go.kr (í¬ë¡¤ë§)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ì´ í™˜ê²½ë³€ìˆ˜: 13ê°œ (í•„ìˆ˜ 7ê°œ + ì„ íƒ 4ê°œ + ê²½ë¡œ 2ê°œ)
í‚¤ ë¶ˆí•„ìš” ì†ŒìŠ¤: 6ê°œ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
""")

# JSON ì €ì¥
report_path = "/Users/twinssn/Projects/aikorea24/api_test/test_gov_ai_report.json"
with open(report_path, 'w', encoding='utf-8') as f:
    json.dump({
        "timestamp": datetime.now().isoformat(),
        "results": results
    }, f, ensure_ascii=False, indent=2)
print(f"ğŸ“„ ë¦¬í¬íŠ¸: {report_path}")
