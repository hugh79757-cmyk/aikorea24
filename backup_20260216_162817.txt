# aikorea24 ë°±ì—… (2026-02-16)
# íŒŒì¼: 6ê°œ
==================================================

## íŒŒì¼
- api_test/requirements.txt
- .gitignore
- api_test/env_template.sh
- api_test/gov_doc_collector.py
- api_test/card_news_generator.py
- api_test/news_collector.py


==================================================
# api_test/requirements.txt
==================================================
requests>=2.31.0
pytrends>=4.9.0
openai>=1.30.0
beautifulsoup4>=4.12.0
Pillow>=10.0.0
python-dotenv>=1.0.0

==================================================
# .gitignore
==================================================
# build output
dist/
# generated types
.astro/

# dependencies
node_modules/

# logs
npm-debug.log*
yarn-debug.log*
yarn-error.log*
pnpm-debug.log*

# environment variables
.env
.env.production

# macOS-specific files
.DS_Store

# jetbrains setting folder
.idea/
api_test/.env.sh

==================================================
# api_test/env_template.sh
==================================================
#!/bin/bash
# ============================================
# aikorea24.kr API í‚¤ ì„¤ì • í…œí”Œë¦¿
# ì´ íŒŒì¼ì„ .env.shë¡œ ë³µì‚¬ í›„ ì‹¤ì œ í‚¤ ì…ë ¥
# cp env_template.sh .env.sh
# ============================================

# [1] ë„¤ì´ë²„ ê°œë°œìì„¼í„° (https://developers.naver.com/apps/)
# - ì‚¬ìš© API: ê²€ìƒ‰(ë‰´ìŠ¤,ë¸”ë¡œê·¸), ë°ì´í„°ë©(ê²€ìƒ‰ì–´íŠ¸ë Œë“œ)
export NAVER_CLIENT_ID=***
export NAVER_CLIENT_SECRET=***

# [2] ë„¤ì´ë²„ ê²€ìƒ‰ê´‘ê³  API (https://manage.searchad.naver.com/)
# - ì‚¬ìš© API: í‚¤ì›Œë“œë„êµ¬ (ì›”ê°„ê²€ìƒ‰ëŸ‰)
export NAVER_AD_API_KEY=***
export NAVER_AD_SECRET=***
export NAVER_AD_CUSTOMER_ID="YOUR_CUSTOMER_ID"

# [3] OpenAI API (https://platform.openai.com/api-keys)
# - ì‚¬ìš© ëª¨ë¸: gpt-4o-mini (ë‰´ìŠ¤ ìš”ì•½)
export OPENAI_API_KEY=***

# [4] ê³µê³µë°ì´í„°í¬í„¸ (https://www.data.go.kr/)
# - ì‚¬ìš© API: ë³´ì¡°ê¸ˆí†µí•©í¬í„¸, ê³µê³µì„œë¹„ìŠ¤(í˜œíƒ)
export DATA_GO_KR_KEY="YOUR_DATA_GO_KR_KEY"

# [5] Instagram Graph API (https://developers.facebook.com/)
# - ì‚¬ìš©: ì¹´ë“œë‰´ìŠ¤ ìë™ ê²Œì‹œ
export INSTAGRAM_ACCESS_TOKEN=***
export INSTAGRAM_BUSINESS_ID="YOUR_IG_BUSINESS_ID"

==================================================
# api_test/gov_doc_collector.py
==================================================
#!/usr/bin/env python3

import os
import sys
import json
import requests
import subprocess
import hashlib
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸
PROJECT_ROOT = '/Users/twinssn/Projects/aikorea24'

# .envì—ì„œ í‚¤ ë¡œë“œ
env_path = os.path.join(PROJECT_ROOT, '.env')
if os.path.exists(env_path):
    with open(env_path) as f:
        for line in f:
            line = line.strip()
            if '=' in line and not line.startswith('#'):
                k, v = line.split('=', 1)
                os.environ[k.strip()] = v.strip()

API_KEY =***
if not API_KEY: ***
    print('ERROR: DATA_GO_KR_KEY not found in .env')
    sys.exit(1)

BASE_URL = 'http://apis.data.go.kr/1741000/publicDoc'

# AI ê´€ë ¨ ê²€ìƒ‰ í‚¤ì›Œë“œ
SEARCH_KEYWORDS = [
    'AI', 'ì¸ê³µì§€ëŠ¥', 'ë””ì§€í„¸', 'ë°ì´í„°', 'í´ë¼ìš°ë“œ',
    'ì†Œí”„íŠ¸ì›¨ì–´', 'ì‚¬ì´ë²„', 'ìŠ¤ë§ˆíŠ¸', 'ììœ¨ì£¼í–‰', 'ë¡œë´‡',
    'ë°˜ë„ì²´', 'GPT', 'ë”¥ëŸ¬ë‹', 'ë¹…ë°ì´í„°', 'ë©”íƒ€ë²„ìŠ¤'
]

# ì—”ë“œí¬ì¸íŠ¸ë³„ ë¬¸ì„œ ìœ í˜•
ENDPOINTS = {
    'getDocPress': 'ë³´ë„ìë£Œ',
    'getDocReport': 'ì •ì±…ë³´ê³ ì„œ',
    'getDocSpeech': 'ì—°ì„¤ë¬¸',
}

# AI ê´€ë ¨ í•„í„°
STRONG_AI_WORDS = ['AI', 'ì¸ê³µì§€ëŠ¥', 'GPT', 'ë”¥ëŸ¬ë‹', 'ë¨¸ì‹ ëŸ¬ë‹', 'LLM', 'ìƒì„±í˜•', 'ì±—ë´‡', 'ChatGPT', 'í´ë¡œë“œ', 'ì•¤íŠ¸ë¡œí”½', 'OpenAI']
WEAK_AI_WORDS = ['ë””ì§€í„¸', 'ë°ì´í„°', 'í´ë¼ìš°ë“œ', 'ìŠ¤ë§ˆíŠ¸', 'ììœ¨ì£¼í–‰', 'ë¡œë´‡', 'ë°˜ë„ì²´', 'ì†Œí”„íŠ¸ì›¨ì–´', 'ì‚¬ì´ë²„', 'ë¹…ë°ì´í„°', 'ë©”íƒ€ë²„ìŠ¤', 'í”Œë«í¼']
EXCLUDE_WORDS = ['ê·€ë†', 'ê·€ì´Œ', 'ê·€ì–´', 'êµë³µ', 'ë¶€ë™ì‚°', 'ì•„íŒŒíŠ¸', 'ì¶•êµ¬', 'ì•¼êµ¬', 'ê²°í˜¼', 'ì´í˜¼', 'ì¥ë¡€']

def is_ai_related(title, text_preview=''):
    combined = (title + ' ' + text_preview[:200]).upper()
    for w in EXCLUDE_WORDS:
        if w in combined:
            return False
    for w in STRONG_AI_WORDS:
        if w.upper() in combined:
            return True
    weak_count = sum(1 for w in WEAK_AI_WORDS if w.upper() in combined)
    return weak_count >= 2

def fetch_docs(endpoint, keyword, num=10, page=1):
    url = f'{BASE_URL}/{endpoint}'
    params = {
        'serviceKey': ***
        'format': 'json',
        'numOfRows': num,
        'pageNo': page,
        'title': keyword
    }
    try:
        resp = requests.get(url, params=params, timeout=15)
        data = resp.json()
        body = data.get('response', {}).get('body', {})
        total = body.get('totalCount', 0)
        results = body.get('resultList', [])
        if isinstance(results, dict):
            results = [results]
        return results, total
    except Exception as e:
        print(f'  Error fetching {endpoint}/{keyword}: {e}')
        return [], 0

def safe_sql(text, maxlen=500):
    if not text:
        return ''
    t = text.replace("'", "''").replace('\\', '').replace('\n', ' ').replace('\r', '')
    return t[:maxlen]

def title_hash(title):
    return hashlib.md5(title.strip().lower().encode()).hexdigest()

def get_existing_hashes():
    cmd = f'npx wrangler d1 execute aikorea24-db --remote --command "SELECT title FROM news;"'
    try:
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, cwd=PROJECT_ROOT, timeout=30)
        hashes = set()
        for line in result.stdout.split('\n'):
            line = line.strip()
            if line and not line.startswith(('â›…', 'â”€', 'ğŸŒ€', 'âœ˜', 'Resource', 'â”Œ', 'â”‚', 'â”œ', 'â””', 'Getting')):
                hashes.add(title_hash(line))
        return hashes
    except:
        return set()

def insert_to_d1(item):
    title = safe_sql(item['title'], 200)
    link = safe_sql(item.get('link', ''), 500)
    desc = safe_sql(item.get('description', ''), 1000)
    source = safe_sql(item.get('source', ''), 50)
    category = safe_sql(item.get('category', 'policy'), 20)
    pub_date = safe_sql(item.get('pub_date', ''), 30)

    sql = f"INSERT INTO news (title, link, description, source, category, pub_date) VALUES ('{title}', '{link}', '{desc}', '{source}', '{category}', '{pub_date}');"
    cmd = f'npx wrangler d1 execute aikorea24-db --remote --command "{sql}"'
    try:
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, cwd=PROJECT_ROOT, timeout=15)
        return result.returncode == 0 and 'ERROR' not in result.stderr
    except:
        return False

def main():
    print('=' * 60)
    print(f'ì •ë¶€ ê³µë¬¸ì„œ AI í•™ìŠµë°ì´í„° ìˆ˜ì§‘ê¸°')
    print(f'ì‹¤í–‰ ì‹œê°: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}')
    print('=' * 60)

    # ê¸°ì¡´ ë‰´ìŠ¤ í•´ì‹œ
    print('\nê¸°ì¡´ ë‰´ìŠ¤ í•´ì‹œ ë¡œë”©...')
    existing = get_existing_hashes()
    print(f'ê¸°ì¡´ ë‰´ìŠ¤: {len(existing)}ê±´')

    collected = []
    for endpoint, doc_type in ENDPOINTS.items():
        print(f'\n--- {doc_type} ({endpoint}) ---')
        ep_count = 0
        for kw in SEARCH_KEYWORDS:
            results, total = fetch_docs(endpoint, kw, num=10, page=1)
            if not results:
                continue
            for item in results:
                if isinstance(item, dict) and 'meta' in item:
                    meta = item['meta']
                    text_data = item.get('data', {}).get('text', '')
                else:
                    meta = item
                    text_data = ''

                title = meta.get('title', '')
                if not title:
                    continue

# ... (42ì¤„ ìƒëµ)

==================================================
# api_test/card_news_generator.py [ìŠ¤í‚µ: 17,526B]

==================================================
# api_test/news_collector.py [ìŠ¤í‚µ: 20,777B]

==================================================
# ì™„ë£Œ: 4ê°œ, ìŠ¤í‚µ: 2ê°œ
